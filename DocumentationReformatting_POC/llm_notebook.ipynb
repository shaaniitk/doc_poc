{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Based Document Processing System\n",
    "\n",
    "Complete document reformatting with actual LLM processing following the modular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Import our LLM document processor\n",
    "from llm_document_processor import LLMDocumentProcessor\n",
    "\n",
    "print(\"LLM Document Processing System loaded successfully\")\n",
    "print(\"This system will actually call LLM APIs for document reformatting\")\n",
    "print(\"Make sure you have MISTRAL_API_KEY or OPENAI_API_KEY in your .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLM Document Processing (Actual Reformatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - modify these as needed\n",
    "input_file = 'bitcoin_whitepaper.tex'\n",
    "template = 'bitcoin_paper'  # 'bitcoin_paper' or 'academic_paper'\n",
    "\n",
    "print(f\"Processing: {input_file}\")\n",
    "print(f\"Template: {template}\")\n",
    "print(\"WARNING: This will make actual LLM API calls and may take several minutes!\")\n",
    "print(\"Each section will be processed individually with the LLM.\")\n",
    "\n",
    "try:\n",
    "    processor = LLMDocumentProcessor()\n",
    "    \n",
    "    # This will actually call the LLM for each section\n",
    "    result = processor.process_document(\n",
    "        source=input_file,\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLLM Processing completed!\")\n",
    "    print(f\"Output: {result['final_document']}\")\n",
    "    print(f\"Sections processed: {result['processed_sections']}\")\n",
    "    print(f\"LLM calls made: {result['analysis']['llm_calls']}\")\n",
    "    print(f\"Quality Score: {result['analysis']['quality_score']:.1f}/100\")\n",
    "    \n",
    "    llm_result = result\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the LLM processing results\n",
    "if 'llm_result' in locals():\n",
    "    print(\"LLM Processing Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    analysis = llm_result['analysis']\n",
    "    \n",
    "    print(f\"Original document:\")\n",
    "    print(f\"  Characters: {analysis['original_elements']['characters']:,}\")\n",
    "    print(f\"  Words: {analysis['original_elements']['words']:,}\")\n",
    "    print(f\"  Equations: {analysis['original_elements']['equations']}\")\n",
    "    \n",
    "    print(f\"\\nProcessed document:\")\n",
    "    print(f\"  Characters: {analysis['final_elements']['characters']:,}\")\n",
    "    print(f\"  Words: {analysis['final_elements']['words']:,}\")\n",
    "    print(f\"  Sections: {analysis['final_elements']['sections']}\")\n",
    "    \n",
    "    print(f\"\\nProcessing metrics:\")\n",
    "    print(f\"  LLM API calls: {analysis['llm_calls']}\")\n",
    "    print(f\"  Quality score: {analysis['quality_score']:.1f}/100\")\n",
    "    print(f\"  Session path: {llm_result['session_path']}\")\n",
    "    \n",
    "    # Show processing log\n",
    "    print(f\"\\nProcessing Log (last 10 entries):\")\n",
    "    for log_entry in analysis['processing_log'][-10:]:\n",
    "        print(f\"  {log_entry}\")\nelse:\n    print(\"No LLM processing results available. Run the processing cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Original vs Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original and processed documents\n",
    "if 'llm_result' in locals():\n",
    "    print(\"Document Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read both documents\n",
    "    with open('bitcoin_whitepaper.tex', 'r') as f:\n",
    "        original = f.read()\n",
    "    \n",
    "    with open(llm_result['final_document'], 'r') as f:\n",
    "        processed = f.read()\n",
    "    \n",
    "    print(f\"Original document:\")\n",
    "    print(f\"  Length: {len(original):,} characters\")\n",
    "    print(f\"  Preview: {original[:200]}...\")\n",
    "    \n",
    "    print(f\"\\nProcessed document:\")\n",
    "    print(f\"  Length: {len(processed):,} characters\")\n",
    "    print(f\"  Preview: {processed[:200]}...\")\n",
    "    \n",
    "    # Show improvement metrics\n",
    "    length_change = (len(processed) - len(original)) / len(original) * 100\n",
    "    print(f\"\\nTransformation metrics:\")\n",
    "    print(f\"  Length change: {length_change:+.1f}%\")\n",
    "    print(f\"  Structure: Original sections -> Organized academic format\")\n",
    "    print(f\"  Enhancement: Raw content -> LLM-refined prose\")\nelse:\n    print(\"No processing results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Individual Section Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze individual processed sections\n",
    "if 'llm_result' in locals():\n",
    "    print(\"Individual Section Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    session_path = llm_result['session_path']\n",
    "    \n",
    "    if os.path.exists(session_path):\n",
    "        files = [f for f in os.listdir(session_path) if f.endswith('.tex') and f != 'final_document.tex']\n",
    "        \n",
    "        print(f\"Generated section files ({len(files)}):\")\n",
    "        \n",
    "        for file in sorted(files):\n",
    "            file_path = os.path.join(session_path, file)\n",
    "            size = os.path.getsize(file_path)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            section_name = file.replace('.tex', '').replace('_', ' ')\n",
    "            word_count = len(content.split())\n",
    "            \n",
    "            print(f\"\\n{section_name}:\")\n",
    "            print(f\"  File: {file}\")\n",
    "            print(f\"  Size: {size:,} bytes\")\n",
    "            print(f\"  Words: {word_count:,}\")\n",
    "            print(f\"  Preview: {content[:150]}...\")\n",
    "    else:\n",
    "        print(\"Session directory not found.\")\nelse:\n    print(\"No processing results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current configuration and available options\n",
    "print(\"System Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load config\n",
    "try:\n",
    "    with open('config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"Current LLM Configuration:\")\n",
    "    print(f\"  Provider: {config['llm']['provider']}\")\n",
    "    print(f\"  Model: {config['llm']['model']}\")\n",
    "    print(f\"  Max tokens: {config['llm']['max_tokens']}\")\n",
    "    print(f\"  Temperature: {config['llm']['temperature']}\")\n",
    "    \n",
    "    print(f\"\\nProcessing Configuration:\")\n",
    "    print(f\"  Template: {config['processing']['template']}\")\n",
    "    print(f\"  Enhancement: {config['processing']['enable_enhancement']}\")\n",
    "    print(f\"  Chunk strategy: {config['processing']['chunk_strategy']}\")\n",
    "    \nexcept FileNotFoundError:\n    print(\"Using default configuration (config.yaml not found)\")\n\nprint(f\"\\nAvailable Templates:\")\nprint(f\"  - bitcoin_paper: Bitcoin whitepaper structure (12 sections)\")\nprint(f\"  - academic_paper: Standard academic format (6 sections)\")\n\nprint(f\"\\nSupported LLM Providers:\")\nprint(f\"  - mistral: Mistral AI (requires MISTRAL_API_KEY)\")\nprint(f\"  - openai: OpenAI (requires OPENAI_API_KEY)\")\n\nprint(f\"\\nEnvironment Variables:\")\nprint(f\"  MISTRAL_API_KEY: {'Set' if os.getenv('MISTRAL_API_KEY') else 'Not set'}\")\nprint(f\"  OPENAI_API_KEY: {'Set' if os.getenv('OPENAI_API_KEY') else 'Not set'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}