#!/usr/bin/env python3
"""
Refactoring Analysis Script
Analyzes the quality and metrics of document refactoring process
"""

import os
import sys
import re
import json
from datetime import datetime

def analyze_refactoring():
    """Analyze refactoring results and save to log file"""
    
    print("🔧 REFACTORING ANALYSIS STARTING...")
    
    # Files to analyze (using available files in export)
    original_file = "bitcoin_whitepaper.tex"  # Using as example unstructured input
    output_file = "final_document.pdf"  # Check for PDF output
    
    # Try to find actual refactored output
    refactored_files = []
    if os.path.exists("latest_run_output"):
        for file in os.listdir("latest_run_output"):
            if file.endswith(".tex"):
                refactored_files.append(os.path.join("latest_run_output", file))
    
    if not refactored_files:
        print("⚠️ No refactored output found, using augmented output as example")
        output_file = "augmented_output.tex"
    else:
        output_file = refactored_files[0]
    
    results = {
        "analysis_type": "refactoring",
        "timestamp": datetime.now().isoformat(),
        "files_analyzed": {
            "original": original_file,
            "output": output_file
        }
    }
    
    # Check if files exist
    if not os.path.exists(original_file):
        print(f"❌ Original file not found: {original_file}")
        return
        
    if not os.path.exists(output_file):
        print(f"❌ Output file not found: {output_file}")
        return
    
    # Read files
    with open(original_file, 'r', encoding='utf-8') as f:
        original_content = f.read()
        
    with open(output_file, 'r', encoding='utf-8') as f:
        output_content = f.read()
    
    print("✅ Files loaded successfully")
    
    # Analyze documents
    original_stats = analyze_document_structure(original_content, "Original")
    output_stats = analyze_document_structure(output_content, "Refactored")
    
    results["original_stats"] = original_stats
    results["output_stats"] = output_stats
    
    # Calculate refactoring metrics
    refactoring_metrics = calculate_refactoring_metrics(original_stats, output_stats)
    results["refactoring_metrics"] = refactoring_metrics
    
    # Structure improvement analysis
    structure_analysis = analyze_structure_improvement(original_content, output_content)
    results["structure_analysis"] = structure_analysis
    
    # Academic formatting analysis
    formatting_analysis = analyze_academic_formatting(output_content)
    results["formatting_analysis"] = formatting_analysis
    
    # Content organization analysis
    organization_analysis = analyze_content_organization(original_content, output_content)
    results["organization_analysis"] = organization_analysis
    
    # Calculate overall quality score
    quality_score = calculate_refactoring_quality_score(results)
    results["overall_quality_score"] = quality_score
    
    # Print summary
    print_refactoring_summary(results)
    
    # Save results to log file
    log_filename = f"refactoring_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(log_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"📊 Analysis saved to: {log_filename}")
    
    return results

def analyze_document_structure(content, doc_type):
    """Analyze document structure and formatting"""
    
    stats = {
        "document_type": doc_type,
        "total_characters": len(content),
        "total_lines": len(content.split('\n')),
        "sections": len(re.findall(r'\\section\{[^}]+\}', content)),
        "subsections": len(re.findall(r'\\subsection\{[^}]+\}', content)),
        "subsubsections": len(re.findall(r'\\subsubsection\{[^}]+\}', content)),
        "paragraphs": len(re.findall(r'\n\s*\n\s*[A-Z]', content)),
        "equations": len(re.findall(r'\\begin\{equation\}.*?\\end\{equation\}', content, re.DOTALL)),
        "figures": len(re.findall(r'\\begin\{figure\}.*?\\end\{figure\}', content, re.DOTALL)),
        "tables": len(re.findall(r'\\begin\{table\}.*?\\end\{table\}', content, re.DOTALL)),
        "itemize_lists": len(re.findall(r'\\begin\{itemize\}.*?\\end\{itemize\}', content, re.DOTALL)),
        "enumerate_lists": len(re.findall(r'\\begin\{enumerate\}.*?\\end\{enumerate\}', content, re.DOTALL)),
        "citations": len(re.findall(r'\\cite\{[^}]+\}', content)),
        "references": len(re.findall(r'\\ref\{[^}]+\}', content)),
        "has_title": bool(re.search(r'\\title\{[^}]+\}', content)),
        "has_author": bool(re.search(r'\\author\{[^}]+\}', content)),
        "has_abstract": bool(re.search(r'\\begin\{abstract\}', content)),
        "has_bibliography": bool(re.search(r'\\begin\{thebibliography\}', content))
    }
    
    print(f"📊 {doc_type} Document Structure:")
    print(f"  Characters: {stats['total_characters']:,}")
    print(f"  Sections: {stats['sections']}")
    print(f"  Subsections: {stats['subsections']}")
    print(f"  Equations: {stats['equations']}")
    print(f"  Academic Elements: Title={stats['has_title']}, Author={stats['has_author']}, Abstract={stats['has_abstract']}")
    
    return stats

def calculate_refactoring_metrics(original_stats, output_stats):
    """Calculate refactoring-specific metrics"""
    
    # Content preservation
    content_preservation = min(100, (output_stats['total_characters'] / original_stats['total_characters']) * 100)
    
    # Structure improvement
    structure_improvement = calculate_structure_improvement_score(original_stats, output_stats)
    
    # Academic formatting improvement
    academic_improvement = calculate_academic_improvement_score(original_stats, output_stats)
    
    # Organization improvement
    organization_improvement = calculate_organization_improvement_score(original_stats, output_stats)
    
    # Technical element preservation
    equation_preservation = calculate_preservation_rate(
        original_stats['equations'], output_stats['equations']
    )
    
    figure_preservation = calculate_preservation_rate(
        original_stats['figures'], output_stats['figures']
    )
    
    table_preservation = calculate_preservation_rate(
        original_stats['tables'], output_stats['tables']
    )
    
    metrics = {
        "content_preservation_percent": round(content_preservation, 2),
        "structure_improvement_score": round(structure_improvement, 2),
        "academic_improvement_score": round(academic_improvement, 2),
        "organization_improvement_score": round(organization_improvement, 2),
        "equation_preservation_percent": round(equation_preservation, 2),
        "figure_preservation_percent": round(figure_preservation, 2),
        "table_preservation_percent": round(table_preservation, 2),
        "overall_improvement_score": round((structure_improvement + academic_improvement + organization_improvement) / 3, 2)
    }
    
    return metrics

def calculate_structure_improvement_score(original_stats, output_stats):
    """Calculate how much the document structure improved"""
    
    score = 0
    
    # More sections generally means better organization
    if output_stats['sections'] > original_stats['sections']:
        score += 20
    elif output_stats['sections'] == original_stats['sections']:
        score += 10
    
    # Hierarchical structure (subsections)
    if output_stats['subsections'] > original_stats['subsections']:
        score += 15
    
    # Proper academic elements
    if output_stats['has_title'] and not original_stats['has_title']:
        score += 15
    if output_stats['has_author'] and not original_stats['has_author']:
        score += 10
    if output_stats['has_abstract'] and not original_stats['has_abstract']:
        score += 20
    if output_stats['has_bibliography'] and not original_stats['has_bibliography']:
        score += 20
    
    return min(100, score)

def calculate_academic_improvement_score(original_stats, output_stats):
    """Calculate academic formatting improvement"""
    
    score = 0
    
    # Academic document elements
    academic_elements = ['has_title', 'has_author', 'has_abstract', 'has_bibliography']
    
    for element in academic_elements:
        if output_stats[element]:
            score += 25
    
    return score

def calculate_organization_improvement_score(original_stats, output_stats):
    """Calculate content organization improvement"""
    
    score = 0
    
    # Better paragraph structure
    if output_stats['paragraphs'] > original_stats['paragraphs']:
        score += 20
    
    # Use of lists for organization
    total_lists_original = original_stats['itemize_lists'] + original_stats['enumerate_lists']
    total_lists_output = output_stats['itemize_lists'] + output_stats['enumerate_lists']
    
    if total_lists_output > total_lists_original:
        score += 15
    
    # Citations and references
    if output_stats['citations'] >= original_stats['citations']:
        score += 15
    
    if output_stats['references'] >= original_stats['references']:
        score += 15
    
    # Hierarchical structure
    if output_stats['subsections'] > 0:
        score += 20
    
    if output_stats['subsubsections'] > 0:
        score += 15
    
    return min(100, score)

def calculate_preservation_rate(original_count, output_count):
    """Calculate preservation rate for specific elements"""
    if original_count == 0:
        return 100.0 if output_count == 0 else 0.0
    return min(100.0, (output_count / original_count) * 100)

def analyze_structure_improvement(original_content, output_content):
    """Analyze structural improvements made during refactoring"""
    
    # Extract section titles
    original_sections = re.findall(r'\\section\{([^}]+)\}', original_content)
    output_sections = re.findall(r'\\section\{([^}]+)\}', output_content)
    
    # Check for standard academic sections
    standard_sections = ['abstract', 'introduction', 'method', 'result', 'discussion', 'conclusion']
    
    original_standard = sum(1 for section in original_sections 
                           if any(std in section.lower() for std in standard_sections))
    output_standard = sum(1 for section in output_sections 
                         if any(std in section.lower() for std in standard_sections))
    
    analysis = {
        "original_section_count": len(original_sections),
        "output_section_count": len(output_sections),
        "original_sections": original_sections,
        "output_sections": output_sections,
        "standard_sections_original": original_standard,
        "standard_sections_output": output_standard,
        "structure_improvement": output_standard > original_standard,
        "sections_added": len(output_sections) - len(original_sections)
    }
    
    return analysis

def analyze_academic_formatting(content):
    """Analyze academic formatting quality"""
    
    # Check for proper LaTeX document structure
    has_documentclass = bool(re.search(r'\\documentclass', content))
    has_packages = len(re.findall(r'\\usepackage', content))
    has_begin_document = bool(re.search(r'\\begin\{document\}', content))
    has_end_document = bool(re.search(r'\\end\{document\}', content))
    has_maketitle = bool(re.search(r'\\maketitle', content))
    
    # Check for proper sectioning
    proper_sectioning = bool(re.search(r'\\section\{[^}]+\}', content))
    
    # Check for mathematical formatting
    proper_math = len(re.findall(r'\\begin\{equation\}', content)) > 0
    
    analysis = {
        "has_documentclass": has_documentclass,
        "package_count": has_packages,
        "has_begin_document": has_begin_document,
        "has_end_document": has_end_document,
        "has_maketitle": has_maketitle,
        "proper_sectioning": proper_sectioning,
        "proper_math_formatting": proper_math,
        "academic_formatting_score": calculate_academic_formatting_score(
            has_documentclass, has_packages, has_begin_document, 
            has_end_document, has_maketitle, proper_sectioning, proper_math
        )
    }
    
    return analysis

def calculate_academic_formatting_score(has_documentclass, has_packages, has_begin_document, 
                                      has_end_document, has_maketitle, proper_sectioning, proper_math):
    """Calculate academic formatting score"""
    
    score = 0
    if has_documentclass: score += 15
    if has_packages > 0: score += 10
    if has_begin_document: score += 15
    if has_end_document: score += 15
    if has_maketitle: score += 10
    if proper_sectioning: score += 20
    if proper_math: score += 15
    
    return score

def analyze_content_organization(original_content, output_content):
    """Analyze how content organization improved"""
    
    # Count organizational elements
    original_org = count_organizational_elements(original_content)
    output_org = count_organizational_elements(output_content)
    
    # Calculate improvement
    improvements = {}
    for key in original_org:
        improvements[key] = output_org[key] - original_org[key]
    
    analysis = {
        "original_organization": original_org,
        "output_organization": output_org,
        "improvements": improvements,
        "overall_organization_improvement": sum(max(0, imp) for imp in improvements.values())
    }
    
    return analysis

def count_organizational_elements(content):
    """Count various organizational elements in content"""
    
    return {
        "sections": len(re.findall(r'\\section\{[^}]+\}', content)),
        "subsections": len(re.findall(r'\\subsection\{[^}]+\}', content)),
        "paragraphs": len(re.findall(r'\n\s*\n\s*[A-Z]', content)),
        "lists": len(re.findall(r'\\begin\{(itemize|enumerate)\}', content)),
        "emphasis": len(re.findall(r'\\(textbf|textit|emph)\{[^}]+\}', content)),
        "cross_references": len(re.findall(r'\\ref\{[^}]+\}', content))
    }

def calculate_refactoring_quality_score(results):
    """Calculate overall quality score for refactoring"""
    
    metrics = results["refactoring_metrics"]
    structure = results["structure_analysis"]
    formatting = results["formatting_analysis"]
    organization = results["organization_analysis"]
    
    # Weighted scoring
    score = 0
    
    # Content preservation (25% weight)
    score += (metrics["content_preservation_percent"] / 100) * 25
    
    # Structure improvement (25% weight)
    score += (metrics["structure_improvement_score"] / 100) * 25
    
    # Academic formatting (20% weight)
    score += (formatting["academic_formatting_score"] / 100) * 20
    
    # Organization improvement (15% weight)
    score += (metrics["organization_improvement_score"] / 100) * 15
    
    # Technical element preservation (15% weight)
    tech_preservation = (metrics["equation_preservation_percent"] + 
                        metrics["figure_preservation_percent"] + 
                        metrics["table_preservation_percent"]) / 3
    score += (tech_preservation / 100) * 15
    
    return round(score, 2)

def print_refactoring_summary(results):
    """Print a summary of the refactoring analysis results"""
    
    print("\n" + "="*60)
    print("🔧 REFACTORING ANALYSIS SUMMARY")
    print("="*60)
    
    metrics = results["refactoring_metrics"]
    structure = results["structure_analysis"]
    formatting = results["formatting_analysis"]
    quality = results["overall_quality_score"]
    
    print(f"🎯 Overall Quality Score: {quality}/100")
    print()
    
    print("📈 Key Improvements:")
    print(f"  Structure Improvement: {metrics['structure_improvement_score']:.1f}/100")
    print(f"  Academic Formatting: {formatting['academic_formatting_score']:.1f}/100")
    print(f"  Organization: {metrics['organization_improvement_score']:.1f}/100")
    print(f"  Content Preservation: {metrics['content_preservation_percent']:.1f}%")
    print()
    
    print("📋 Document Transformation:")
    original = results["original_stats"]
    output = results["output_stats"]
    print(f"  Sections: {original['sections']} → {output['sections']} ({structure['sections_added']:+d})")
    print(f"  Academic Elements Added:")
    if output['has_title'] and not original['has_title']: print("    ✅ Title")
    if output['has_author'] and not original['has_author']: print("    ✅ Author")
    if output['has_abstract'] and not original['has_abstract']: print("    ✅ Abstract")
    if output['has_bibliography'] and not original['has_bibliography']: print("    ✅ Bibliography")
    print()
    
    print("🎨 Formatting Improvements:")
    print(f"  LaTeX Structure: {'✅' if formatting['has_documentclass'] else '❌'}")
    print(f"  Proper Sectioning: {'✅' if formatting['proper_sectioning'] else '❌'}")
    print(f"  Math Formatting: {'✅' if formatting['proper_math_formatting'] else '❌'}")
    
    print("\n" + "="*60)

if __name__ == "__main__":
    analyze_refactoring()