"""
Hierarchy-Aware Output Management and Document Aggregation.

This module is responsible for saving all processing outputs. It can save the
content of each individual node in a processed tree and can aggregate the
final, processed document tree into a valid, well-formatted output file.
"""
import os
from datetime import datetime
from config import OUTPUT_FORMATS
import json 
import re

class OutputManager:
    def __init__(self, base_path="outputs"):
        self.base_path = base_path
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_path = os.path.join(base_path, self.session_id)
        os.makedirs(self.session_path, exist_ok=True)
        # Create a dedicated subfolder for the individual node outputs
        self.nodes_path = os.path.join(self.session_path, "processed_nodes")
        os.makedirs(self.nodes_path, exist_ok=True)

    # --- NEW: HIERARCHICAL NODE SAVING ---
    def save_processed_tree_nodes(self, processed_tree):
        """
        Saves the processed content of every node in the tree as a separate file.

        This is a valuable feature for debugging and reviewing the output of
        the LLM at each step of the document's hierarchy.

        Args:
            processed_tree (dict): The final, processed hierarchical document tree.
        """
        # Start the recursive saving process at the top level of the tree.
        self._save_node_recursively(processed_tree, path_parts=[])

    def _save_node_recursively(self, node_level, path_parts):
        """
        The core recursive engine for saving each node's content.
        """
        for title, node_data in node_level.items():
            # This check gracefully handles special, non-dictionary items
            # like the 'Orphaned_Content' list, preventing the crash.
            if not isinstance(node_data, dict):
                continue
            current_path = path_parts + [title]
            
            # If the node has processed content, save it.
            content_to_save = node_data.get('processed_content')
            if content_to_save:
                # Create a descriptive filename from the hierarchical path.
                # e.g., ['Section 1', 'Subsection 1.1'] -> "Section_1_Subsection_1.1.tex"
                safe_title = title.replace(" ", "_").replace(".", "")
                filename = "_".join([p.replace(" ", "_").replace(".", "") for p in path_parts] + [safe_title]) + ".tex"
                file_path = os.path.join(self.nodes_path, filename)

                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content_to_save)

            # Recurse into the subsections, passing down the updated path.
            if node_data.get('subsections'):
                self._save_node_recursively(node_data['subsections'], path_parts=current_path)

    # --- AGGREGATION & LOGGING (No changes to the methods below) ---
    def aggregate_document(self, final_document_string, output_format="latex"):
        """
        Saves the final, fully formatted document string to a file.
        This method is now simpler: it just performs the file write operation.
        """
        format_config = OUTPUT_FORMATS.get(output_format, OUTPUT_FORMATS['latex'])
        final_path = os.path.join(self.session_path, f"final_document{format_config['extension']}")
        
        with open(final_path, 'w', encoding='utf-8') as f:
            f.write(final_document_string)
        
        return final_path

    def save_processing_log(self, log_entries):
        """Save a log of all processing steps for traceability."""
        log_path = os.path.join(self.session_path, "processing_log.txt")
        
        with open(log_path, 'w', encoding='utf-8') as f:
            for entry in log_entries:
                f.write(f"{entry}\n")
        
        return log_path
    
    def save_json_output(self, filename, data):
        """Saves a Python dictionary or list to a formatted JSON file."""
        file_path = os.path.join(self.session_path, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            # Use indent=2 for nice, human-readable formatting
            json.dump(data, f, indent=2)
        return file_path

    def save_generic_output(self, filename, content):
        """Saves a generic string content to a file in the session path."""
        file_path = os.path.join(self.session_path, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return file_path


def final_latex_sanitization(document_string: str) -> str:
    """
    A final, aggressive sanitization pass to remove nested LaTeX document structures
    that may have been generated by the LLM.
    """
    # Preserve the top-level LaTeX document, sanitize only nested documents inside the body
    start_marker = "\\begin{document}"
    end_marker = "\\end{document}"

    start_idx = document_string.find(start_marker)
    end_idx = document_string.rfind(end_marker)

    # If not a full LaTeX document or malformed, do nothing
    if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:
        return document_string

    preamble_and_begin = document_string[:start_idx]
    body_content = document_string[start_idx + len(start_marker): end_idx]
    tail = document_string[end_idx:]  # includes \end{document}

    # Remove any fully nested LaTeX documents within the body only
    nested_doc_pattern = re.compile(r"\\documentclass.*?\\begin{document}(.*?)\\end{document}", re.DOTALL)

    def extract_content(match):
        return match.group(1).strip()

    body_content = nested_doc_pattern.sub(extract_content, body_content)

    # Remove any stray \documentclass lines that accidentally appeared inside the body
    body_content = re.sub(r"\\documentclass\{.*?\}", "", body_content)

    # Reassemble preserving the original preamble and outer document structure
    return preamble_and_begin + start_marker + body_content + tail